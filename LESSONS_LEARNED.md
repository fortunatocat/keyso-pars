# Keys.so Parser — Разбор полётов: сложности и решения

> Описывает все нетривиальные проблемы, с которыми столкнулись при разработке,
> и как именно из каждой выходили.

---

## Контекст задачи

Нужно было обходить ~23 000 доменов через Keys.so API и дописывать SEO-метрики
(топ-1/3/5/10/50, трафик) в CSV-файл.
Звучит просто. Оказалось — нет.

---

## Проблема 1. API готовит отчёт асинхронно — и молчит об этом

**Симптом:**
POST `/report/group` принимает запрос и возвращает `{"rid": "abc123"}` мгновенно.
Но данные ещё не готовы. GET `/report/group/domains/{rid}` возвращает HTTP **200**
с телом `{"code": 202, "message": "Отчёт формируется..."}`.

Это нестандартное поведение: HTTP 200 при фактически незавершённом ответе.
Первая реализация принимала это за успех и писала пустые строки в CSV.

**Решение:**
- Явно разобрать тело ответа: если `data["code"] == 202` — это sentinel `_NOT_READY`,
  а не результат.
- Ввести `pending_retry` — словарь rid → время первой попытки.
  При `_NOT_READY` — повторять каждые `NOT_READY_RETRY_WAIT=60s`
  до `NOT_READY_TOTAL_TIMEOUT=60s`.
- Если таймаут истёк — дропать rid: completion check подберёт домены позже.

---

## Проблема 2. batch_size=200 → HTTP 410, не 400

**Симптом:**
Попытка ускорить обработку увеличением батча до 200 доменов.
API отвечал HTTP **410 Gone** с первого же запроса, без объяснений.

**Решение:**
Эмпирически установлено: рабочий максимум — **100 доменов на батч**.
Жёстко задокументировано в коде:
```python
parser.add_argument("--batch-size", type=int, default=100,
    help="Domains per API batch (max ~100; 200+ returns HTTP 410)")
```
Константа `MIN_BATCH_FOR_GROUP = 10` — минимум для group endpoint.

---

## Проблема 3. Малые батчи вызывают POST 400

**Симптом:**
Completion check и error retry работают с остатками — иногда 1–9 доменов.
POST `/report/group` при таком батче возвращает HTTP **400**
с сообщением «требуется более одного домена».

**Первая попытка:**
Обрабатывать 400 как временную ошибку и повторять POST. Не работает —
ошибка детерминированная, не временная.

**Решение:**
Ввести sentinel `_USE_FALLBACK`. Если POST вернул 400 с "одного домена" —
немедленно переключаться на `GET /report/simple/domain_dashboard`
(один запрос на домен, синхронно).
```python
if r.status_code == 400 and "одного домена" in r.text:
    return _USE_FALLBACK
```
Pipeline при получении `_USE_FALLBACK` вызывает `_process_fallback_batch()` сразу,
не ждёт `INITIAL_WAIT`.

---

## Проблема 4. Simple endpoint тоже возвращает code=202

**Симптом:**
После добавления fallback на simple endpoint — часть доменов
писалась с пустыми метриками и ошибкой "not found".
Оказалось: `/report/simple/domain_dashboard` тоже может вернуть
`{"code": 202, "message": "..."}` в теле HTTP 200.

**Решение:**
В `get_single_domain_metrics()` добавить проверку:
```python
if str(code) == "202":
    return _NOT_READY
```
В fallback-батче `_NOT_READY` трактуется как `None` (домен не найден, быстрый выход).
Инлайн-retry для 202 в fallback **намеренно не делался** — см. Проблему 5.

---

## Проблема 5. Fallback-retry превратил 3 минуты в 19 минут

**Симптом:**
Тестовый прогон 500 доменов:
- Первая версия (wave mode): 7 мин.
- После pipeline-рефакторинга: **19 минут** — стало хуже в 2.5×!

**Анализ:**
`run_error_retry` читал output CSV, находил строки с любой ошибкой,
удалял их и перегонял заново через pipeline.
Среди «ошибок» были строки с `error = "not found in report"`.
Это ~34 домена, которые Keys.so просто не индексирует.

При перегоне этих 34 доменов:
1. POST `/report/group` → HTTP 400 «одного домена» (все домены не в индексе).
2. Fallback: 34 × sequential GET `/report/simple/domain_dashboard`.
3. Каждый запрос — **10–40 секунд** (server timeout для неиндексированных доменов).
4. Итого: 34 × ~18s = **~10 минут только на fallback**.

Это происходило при каждом error retry — бесконечный цикл деградации.

**Ключевое понимание:**
`"not found in report"` — это **валидный финальный результат**, а не ошибка.
Домен просто отсутствует в базе Keys.so. Повторять такие запросы бессмысленно.

**Решение:**
```python
NOT_FOUND = "not found in report"
keep_rows  = [r for r in error_rows if error == NOT_FOUND]   # оставить как есть
retry_rows = [r for r in error_rows if error != NOT_FOUND]   # только реальные ошибки
```
Если `retry_rows` пуст — error retry завершается мгновенно.
Fallback-батч: **один проход, без инлайн-retry**.

Результат после фикса: **3 мин 11 сек** вместо 19 минут.

---

## Проблема 6. Wave-архитектура — структурный простой ~47 минут

**Симптом:**
Первоначальная архитектура «волн»:
1. Fire-phase: отправить N батчей с интервалом 10s.
2. Wait-phase: ждать до INITIAL_WAIT=90s.
3. Collect-phase: собрать результаты.
4. Следующая волна.

**Математика потерь:**
```
batch_size  = 100 доменов
wave_size   = 90 // 10 = 9 батчей/волна

Fire:    8 × 10s = 80s
Wait:    90 - 80 = 10s
Collect: ~28s (включая Throttle-штраф — см. ниже)
─────────────────
Одна волна: ~118s
35k доменов: 350 батчей → 39 волн × 118s ≈ 77 минут
```

Проблема: пока волна N в collect-фазе, сервер **простаивает**.
Волна N+1 не начинается, пока N полностью не завершена.

**Решение — true pipeline:**
Убрать понятие «волны». Вместо этого:
- Каждые `BATCH_INTERVAL=5s`: POST следующего батча.
- Параллельно: если с момента POST прошло ≥ `INITIAL_WAIT=90s` — GET результата.
- Все батчи перекрываются во времени на сервере.

```
t=0:   POST batch_1
t=5:   POST batch_2
...
t=85:  POST batch_18
t=90:  GET batch_1   ← 90s прошло с moment POST
t=95:  POST batch_19 + GET batch_2
...
```

**Результат:**
```
Batch 350: fire t=1745s, collect t=1835s
Итого: ~30 минут  (было 77 — ускорение 2.5×)
```

---

## Проблема 7. Throttle съедал 20 секунд на каждую волну

**Симптом:**
Класс `Throttle` (10 req/10s) считал и POST, и GET в один счётчик.
За fire-phase тратилось 9 тиков (9 POST'ов).
Первый GET = 10-й тик → принудительный `sleep(20s)`.

Но POST'ы уже разнесены `BATCH_INTERVAL`'ом — они и так не нарушают лимит.
Throttle между fire и collect — **чистые потери**: 20s × 39 волн = **13 минут**.

**Решение:**
Удалить класс `Throttle` полностью.
В pipeline-архитектуре rate-limiting не нужен:
- POST: естественный интервал `BATCH_INTERVAL=5s` → 2 req/10s, запас ×5.
- GET: один раз в те же 5s → суммарно 4 req/10s — далеко от лимита 10/10s.

---

## Проблема 8. BATCH_INTERVAL=10s — запас в 10 раз больше нужного

**Симптом:**
API-лимит: 10 req/10s. POST раз в 10s = 1 req/10s — запас ×10.

**Решение:**
`BATCH_INTERVAL: 10 → 5`.
При 5s: 2 req/10s → запас ×5. Полностью безопасно.
В wave-mode это удваивало wave_size: 9 → 18 батчей/волна.
В pipeline-mode — ускоряет fire-фазу вдвое.

---

## Проблема 9. NOT_READY_TOTAL_TIMEOUT=900s — ненужное ожидание

**Симптом:**
При 202-ответе скрипт мог ждать до 15 минут одного rid,
прежде чем дропнуть его в completion check.

На практике Keys.so готовит отчёты за 60–90s.
Если за 60s не готово — что-то пошло не так, и дальнейшее ожидание бессмысленно.

**Решение:**
`NOT_READY_TOTAL_TIMEOUT: 900 → 60`.
Rid дропается через 60s, домены подберёт completion check.

---

## Проблема 10. Двойной вызов `_print_final_count`

**Симптом:**
В лог дважды выводилась итоговая статистика доменов —
один раз из `run_completion_check()`, один раз из `main()`.

**Решение:**
Удалить вызов из `run_completion_check()`, оставить только в `main()`.
Простая невнимательность при рефакторинге.

---

## Итоговые цифры

| Версия | Время (500 доменов) | Экстраполяция 35k |
|---|---|---|
| Wave mode + Throttle | ~7 мин | ~77 мин |
| Pipeline (первая попытка с fallback-retry) | **19 мин** | ~133 мин ❌ |
| Pipeline + фикс error_retry | **3 мин 11 сек** | ~26 мин ✅ |

---

## Главный урок

> **Самая опасная ошибка** — считать `"not found in report"` временной ошибкой.
> Это было корнем деградации производительности: retry неиндексированных доменов
> гарантированно вёл к sequential fallback по 10–40 секунд на домен.
>
> В API, где «не найдено» = HTTP 200 с кодом в теле — семантику ответа
> нужно разбирать явно, а не полагаться на HTTP-статус.
